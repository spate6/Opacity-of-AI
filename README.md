# Opacity-of-AI
*A brief position statement summarizing my thoughts on Opacity of AI*

With the increasing prevalence of AI and its applications in various fields, there is a growing concern regarding the transparency of these algorithms. While some forms of Explainable AI have been developed to enable human comprehension and informed decision-making, there remains a significant issue with powerful AI algorithms, such as Neural Networks, which are inherently opaque. As highlighted by Josh Clark in a podcast, opaque AI operates without human monitoring, as the rules of the algorithm are not explicitly coded by humans.

From personal experience, I have observed instances where AI, like Google Maps, fails to provide the most optimal routes. This raises questions about the accuracy and efficacy of AI solutions. How can we be certain that the outcomes generated by AI algorithms are truly accurate and the best possible solutions? As emphasized by Jaakkola and Tom Gruber, understanding the reasoning behind AI decision-making is imperative. However, as AI algorithms progress from Illiterate Opacity to Intrinsic Opacity, it becomes increasingly challenging for even the creators of these algorithms to decipher their inner workings. Consequently, we are unable to fine-tune or comprehend their decision-making processes, which poses ethical concerns.

In my opinion, every technology created by humans can be explained, except for opaque AI. This lack of transparency unsettles me because the inner workings define the essence of AI. There is a saying, "It's what is on the inside that matters," and yet, when it comes to opaque AI, we are left in the dark, unable to judge the true nature of the technology.

Transparency is essential, especially when dealing with technologies that are not yet robust enough and are prone to making mistakes. It raises intriguing questions as to whether these concerns would change if a Neural Network achieved 100% accuracy. Perhaps they would not. Consequently, any form of opaque AI should not exist unless there is significant research development that provides a complete understanding of its inner workings. Placing our trust in a system that could potentially endanger lives and our collective future is unacceptable.

To further emphasize this point, consider the unsettling example of an autonomous vehicle making a wrong maneuver that could harm both passengers and pedestrians. Can we truly trust opaque AI, even if its algorithm improves by learning from human casualties?

In conclusion, the opacity of AI poses significant ethical challenges. Complete transparency must be ensured for any technology that lacks robustness and has the potential to make mistakes. Without a thorough understanding of the inner workings of AI algorithms, we cannot place our trust in systems that have the capacity to impact lives and shape our future.
